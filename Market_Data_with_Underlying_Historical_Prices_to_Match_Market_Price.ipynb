{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn import svm\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['MMM','AXP','AAPL','BA','CAT','CVX','CSCO','KO','DWDP','XOM',\n",
    "             'GE','GS','HD','IBM','INTC','JNJ','JPM','MCD','MRK','MSFT',\n",
    "             'NKE','PFE','PG','TRV','UNH','UTX','VZ','V','WMT','DIS']\n",
    "\n",
    "df = None\n",
    "for company in companies:\n",
    "    temp_df = pd.read_excel('data\\{}.xlsx'.format(company,na_value='nan'))\n",
    "    if df is None:\n",
    "        df = temp_df\n",
    "    else:\n",
    "        df = df.append(temp_df,ignore_index = True)\n",
    "    \n",
    "# rename variables\n",
    "df = df.rename(index=str,columns={'OptType':'CallPut','Spot':'S0','Strike':'K','Rate':'rd','DvYd':'q','IVM':'sigma','DyEx':'T','Mid':'Option Price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Varaibles: \n",
      "['CallPut' 'S0' 'K' 'rd' 'q' 'T' 'S249' 'S248' 'S247' 'S246' 'S245' 'S244'\n",
      " 'S243' 'S242' 'S241' 'S240' 'S239' 'S238' 'S237' 'S236' 'S235' 'S234'\n",
      " 'S233' 'S232' 'S231' 'S230' 'S229' 'S228' 'S227' 'S226' 'S225' 'S224'\n",
      " 'S223' 'S222' 'S221' 'S220' 'S219' 'S218' 'S217' 'S216' 'S215' 'S214'\n",
      " 'S213' 'S212' 'S211' 'S210' 'S209' 'S208' 'S207' 'S206' 'S205' 'S204'\n",
      " 'S203' 'S202' 'S201' 'S200' 'S199' 'S198' 'S197' 'S196' 'S195' 'S194'\n",
      " 'S193' 'S192' 'S191' 'S190' 'S189' 'S188' 'S187' 'S186' 'S185' 'S184'\n",
      " 'S183' 'S182' 'S181' 'S180' 'S179' 'S178' 'S177' 'S176' 'S175' 'S174'\n",
      " 'S173' 'S172' 'S171' 'S170' 'S169' 'S168' 'S167' 'S166' 'S165' 'S164'\n",
      " 'S163' 'S162' 'S161' 'S160' 'S159' 'S158' 'S157' 'S156' 'S155' 'S154'\n",
      " 'S153' 'S152' 'S151' 'S150' 'S149' 'S148' 'S147' 'S146' 'S145' 'S144'\n",
      " 'S143' 'S142' 'S141' 'S140' 'S139' 'S138' 'S137' 'S136' 'S135' 'S134'\n",
      " 'S133' 'S132' 'S131' 'S130' 'S129' 'S128' 'S127' 'S126' 'S125' 'S124'\n",
      " 'S123' 'S122' 'S121' 'S120' 'S119' 'S118' 'S117' 'S116' 'S115' 'S114'\n",
      " 'S113' 'S112' 'S111' 'S110' 'S109' 'S108' 'S107' 'S106' 'S105' 'S104'\n",
      " 'S103' 'S102' 'S101' 'S100' 'S99' 'S98' 'S97' 'S96' 'S95' 'S94' 'S93'\n",
      " 'S92' 'S91' 'S90' 'S89' 'S88' 'S87' 'S86' 'S85' 'S84' 'S83' 'S82' 'S81'\n",
      " 'S80' 'S79' 'S78' 'S77' 'S76' 'S75' 'S74' 'S73' 'S72' 'S71' 'S70' 'S69'\n",
      " 'S68' 'S67' 'S66' 'S65' 'S64' 'S63' 'S62' 'S61' 'S60' 'S59' 'S58' 'S57'\n",
      " 'S56' 'S55' 'S54' 'S53' 'S52' 'S51' 'S50' 'S49' 'S48' 'S47' 'S46' 'S45'\n",
      " 'S44' 'S43' 'S42' 'S41' 'S40' 'S39' 'S38' 'S37' 'S36' 'S35' 'S34' 'S33'\n",
      " 'S32' 'S31' 'S30' 'S29' 'S28' 'S27' 'S26' 'S25' 'S24' 'S23' 'S22' 'S21'\n",
      " 'S20' 'S19' 'S18' 'S17' 'S16' 'S15' 'S14' 'S13' 'S12' 'S11' 'S10' 'S9'\n",
      " 'S8' 'S7' 'S6' 'S5' 'S4' 'S3' 'S2' 'S1']\n",
      "Output Variable\n",
      "Option Price\n"
     ]
    }
   ],
   "source": [
    "# drop Ticker and Volm to make the dataset\n",
    "dataset = df.drop(labels=['Ticker','Volm','sigma'],axis=1)\n",
    "#dataset = dataset[['CallPut','K','rd','q','T','S0','S1','S2','S3','S4','S5','S6','Option Price']]\n",
    "#dataset = dataset[['CallPut','K','rd','q','T','S0','Option Price']]\n",
    "\n",
    "# column names\n",
    "colname = dataset.columns.values\n",
    "print('Input Varaibles: ')\n",
    "print(dataset.columns.values[:-1])\n",
    "print(\"Output Variable: \")\n",
    "print(dataset.columns.values[-1])\n",
    "\n",
    "# drop sigma=0\n",
    "#dataset = dataset[dataset['sigma']!=0]\n",
    "# transform T to year\n",
    "dataset['T'] = dataset['T']/365\n",
    "# change rd,q,sigma into percent\n",
    "dataset['rd'] = dataset['rd']/100\n",
    "dataset['q'] = dataset['q']/100\n",
    "#dataset['sigma'] = dataset['sigma']/100\n",
    "\n",
    "# transform dataframe into numpy array and shuffle the data\n",
    "dataset = np.array(dataset)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "X = dataset[:,:-1]\n",
    "Y = dataset[:,-1].reshape(-1,1)\n",
    "\n",
    "# normalize X\n",
    "X = (X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "#X = (X-np.mean(X,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "num_test = int(test_ratio*dataset.shape[0])\n",
    "X_test = X[:num_test]\n",
    "X_train = X[num_test:]\n",
    "Y_test = Y[:num_test]\n",
    "Y_train = Y[num_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 256)               65536     \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 197,377\n",
      "Trainable params: 197,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9081 samples, validate on 2271 samples\n",
      "Epoch 1/1000\n",
      "9081/9081 [==============================] - 2s 222us/step - loss: 669.0971 - val_loss: 589.8738\n",
      "Epoch 2/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 607.1191 - val_loss: 526.1922\n",
      "Epoch 3/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 519.9113 - val_loss: 349.9102\n",
      "Epoch 4/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 422.9409 - val_loss: 408.6868\n",
      "Epoch 5/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 369.3045 - val_loss: 208.7340\n",
      "Epoch 6/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 326.1424 - val_loss: 217.4923\n",
      "Epoch 7/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 272.3000 - val_loss: 518.7783\n",
      "Epoch 8/1000\n",
      "9081/9081 [==============================] - 1s 120us/step - loss: 261.6587 - val_loss: 104.6113\n",
      "Epoch 9/1000\n",
      "9081/9081 [==============================] - 1s 119us/step - loss: 223.6565 - val_loss: 70.0871\n",
      "Epoch 10/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 224.9345 - val_loss: 101.2526\n",
      "Epoch 11/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 197.4423 - val_loss: 88.7098\n",
      "Epoch 12/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 200.0712 - val_loss: 57.7343\n",
      "Epoch 13/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 176.6625 - val_loss: 103.7681\n",
      "Epoch 14/1000\n",
      "9081/9081 [==============================] - 1s 85us/step - loss: 175.6842 - val_loss: 378.0434\n",
      "Epoch 15/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 183.6258 - val_loss: 47.2014\n",
      "Epoch 16/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 156.9929 - val_loss: 120.8823\n",
      "Epoch 17/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 147.3771 - val_loss: 44.6283\n",
      "Epoch 18/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 141.6513 - val_loss: 37.0447\n",
      "Epoch 19/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 134.7045 - val_loss: 38.4794\n",
      "Epoch 20/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 128.4470 - val_loss: 144.9361\n",
      "Epoch 21/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 122.8132 - val_loss: 130.4579\n",
      "Epoch 22/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 120.7581 - val_loss: 63.0136\n",
      "Epoch 23/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 103.9455 - val_loss: 305.9478\n",
      "Epoch 24/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 109.3959 - val_loss: 52.0960\n",
      "Epoch 25/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 115.8569 - val_loss: 78.4690\n",
      "Epoch 26/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 100.5726 - val_loss: 32.5930\n",
      "Epoch 27/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 93.4229 - val_loss: 160.4412\n",
      "Epoch 28/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 93.8715 - val_loss: 53.2387\n",
      "Epoch 29/1000\n",
      "9081/9081 [==============================] - 1s 82us/step - loss: 92.8125 - val_loss: 62.4532\n",
      "Epoch 30/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 88.2263 - val_loss: 54.0042\n",
      "Epoch 31/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 91.0515 - val_loss: 141.4378\n",
      "Epoch 32/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 96.4792 - val_loss: 24.9545\n",
      "Epoch 33/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 80.7623 - val_loss: 24.6471\n",
      "Epoch 34/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 78.8499 - val_loss: 40.3127\n",
      "Epoch 35/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 81.8056 - val_loss: 156.0586\n",
      "Epoch 36/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 76.3885 - val_loss: 135.0305\n",
      "Epoch 37/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 75.6918 - val_loss: 120.4034\n",
      "Epoch 38/1000\n",
      "9081/9081 [==============================] - 1s 82us/step - loss: 72.6496 - val_loss: 405.3712\n",
      "Epoch 39/1000\n",
      "9081/9081 [==============================] - 2s 171us/step - loss: 79.9467 - val_loss: 26.6843\n",
      "Epoch 40/1000\n",
      "9081/9081 [==============================] - 2s 189us/step - loss: 65.0532 - val_loss: 26.2463\n",
      "Epoch 41/1000\n",
      "9081/9081 [==============================] - 1s 149us/step - loss: 73.8402 - val_loss: 110.8147\n",
      "Epoch 42/1000\n",
      "9081/9081 [==============================] - 1s 128us/step - loss: 67.6801 - val_loss: 135.0495\n",
      "Epoch 43/1000\n",
      "9081/9081 [==============================] - 1s 130us/step - loss: 67.3617 - val_loss: 35.5249\n",
      "Epoch 44/1000\n",
      "9081/9081 [==============================] - 1s 133us/step - loss: 62.2919 - val_loss: 40.0859\n",
      "Epoch 45/1000\n",
      "9081/9081 [==============================] - 1s 123us/step - loss: 62.3607 - val_loss: 20.9670\n",
      "Epoch 46/1000\n",
      "9081/9081 [==============================] - 1s 126us/step - loss: 57.2162 - val_loss: 23.3855\n",
      "Epoch 47/1000\n",
      "9081/9081 [==============================] - 1s 126us/step - loss: 59.9940 - val_loss: 37.5362\n",
      "Epoch 48/1000\n",
      "9081/9081 [==============================] - 1s 122us/step - loss: 58.9209 - val_loss: 51.9425\n",
      "Epoch 49/1000\n",
      "9081/9081 [==============================] - 853s 94ms/step - loss: 65.7378 - val_loss: 15.3299\n",
      "Epoch 50/1000\n",
      "9081/9081 [==============================] - 1s 82us/step - loss: 56.5530 - val_loss: 17.7961\n",
      "Epoch 51/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 60.3023 - val_loss: 30.2692\n",
      "Epoch 52/1000\n",
      "9081/9081 [==============================] - 1s 65us/step - loss: 54.3338 - val_loss: 16.1364\n",
      "Epoch 53/1000\n",
      "9081/9081 [==============================] - 0s 54us/step - loss: 52.4418 - val_loss: 93.8129\n",
      "Epoch 54/1000\n",
      "9081/9081 [==============================] - 0s 44us/step - loss: 51.6567 - val_loss: 39.3236\n",
      "Epoch 55/1000\n",
      "9081/9081 [==============================] - 0s 48us/step - loss: 50.2025 - val_loss: 18.9732\n",
      "Epoch 56/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 48.6387 - val_loss: 177.0802\n",
      "Epoch 57/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 51.4058 - val_loss: 51.9929\n",
      "Epoch 58/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 48.7827 - val_loss: 45.4459\n",
      "Epoch 59/1000\n",
      "9081/9081 [==============================] - ETA: 0s - loss: 50.65 - 1s 72us/step - loss: 53.3975 - val_loss: 37.4494\n",
      "Epoch 60/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 43.8803 - val_loss: 169.8551\n",
      "Epoch 61/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 48.8623 - val_loss: 82.8217\n",
      "Epoch 62/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 47.1367 - val_loss: 15.8450\n",
      "Epoch 63/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 44.7886 - val_loss: 14.8304\n",
      "Epoch 64/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 49.0759 - val_loss: 25.6874\n",
      "Epoch 65/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 45.0895 - val_loss: 14.0314\n",
      "Epoch 66/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 45.4876 - val_loss: 36.1736\n",
      "Epoch 67/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 41.8101 - val_loss: 72.5622\n",
      "Epoch 68/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 43.3972 - val_loss: 49.6769\n",
      "Epoch 69/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 37.1270 - val_loss: 260.2188\n",
      "Epoch 70/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 43.2535 - val_loss: 16.4981\n",
      "Epoch 71/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 44.0168 - val_loss: 77.9249\n",
      "Epoch 72/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 41.6624 - val_loss: 21.4168\n",
      "Epoch 73/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 37.7427 - val_loss: 26.8223\n",
      "Epoch 74/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 38.0665 - val_loss: 48.5830\n",
      "Epoch 75/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 39.9817 - val_loss: 15.9182\n",
      "Epoch 76/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 39.8544 - val_loss: 11.5934\n",
      "Epoch 77/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 38.9803 - val_loss: 13.1982\n",
      "Epoch 78/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 35.8215 - val_loss: 11.4331\n",
      "Epoch 79/1000\n",
      "9081/9081 [==============================] - 1s 92us/step - loss: 40.2972 - val_loss: 15.1951\n",
      "Epoch 80/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 36.0292 - val_loss: 12.3993\n",
      "Epoch 81/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 36.7077 - val_loss: 21.5520\n",
      "Epoch 82/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 34.3321 - val_loss: 39.5871\n",
      "Epoch 83/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 35.9584 - val_loss: 17.2429\n",
      "Epoch 84/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 34.1283 - val_loss: 70.9305\n",
      "Epoch 85/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 35.3325 - val_loss: 26.0813\n",
      "Epoch 86/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 33.7847 - val_loss: 51.4946\n",
      "Epoch 87/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 32.2681 - val_loss: 90.8493\n",
      "Epoch 88/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 35.8652 - val_loss: 25.6668\n",
      "Epoch 89/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 33.0420 - val_loss: 8.4058\n",
      "Epoch 90/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 30.9495 - val_loss: 50.4870\n",
      "Epoch 91/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 34.7172 - val_loss: 16.6361\n",
      "Epoch 92/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 33.8446 - val_loss: 43.8613\n",
      "Epoch 93/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 32.1455 - val_loss: 9.3291\n",
      "Epoch 94/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 34.5624 - val_loss: 10.3209\n",
      "Epoch 95/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 31.4897 - val_loss: 58.3545\n",
      "Epoch 96/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 32.2782 - val_loss: 8.4258\n",
      "Epoch 97/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 29.4506 - val_loss: 41.2751\n",
      "Epoch 98/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 31.5014 - val_loss: 6.8537\n",
      "Epoch 99/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 29.5576 - val_loss: 58.8081\n",
      "Epoch 100/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 30.4758 - val_loss: 81.4515\n",
      "Epoch 101/1000\n",
      "9081/9081 [==============================] - 1s 61us/step - loss: 31.7070 - val_loss: 15.8513\n",
      "Epoch 102/1000\n",
      "9081/9081 [==============================] - 1s 63us/step - loss: 29.4542 - val_loss: 10.8803\n",
      "Epoch 103/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 30.4371 - val_loss: 18.8050\n",
      "Epoch 104/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 32.6672 - val_loss: 17.5894\n",
      "Epoch 105/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 24.0276 - val_loss: 114.2036\n",
      "Epoch 106/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 30.4446 - val_loss: 91.9211\n",
      "Epoch 107/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 29.2535 - val_loss: 47.7180\n",
      "Epoch 108/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 29.2873 - val_loss: 24.5455\n",
      "Epoch 109/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 27.5611 - val_loss: 9.5245\n",
      "Epoch 110/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 31.0958 - val_loss: 6.7927\n",
      "Epoch 111/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 26.8515 - val_loss: 30.3989\n",
      "Epoch 112/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 28.1589 - val_loss: 46.7595\n",
      "Epoch 113/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 27.1302 - val_loss: 36.6584\n",
      "Epoch 114/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 25.5433 - val_loss: 13.5394\n",
      "Epoch 115/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 28.4579 - val_loss: 8.8171\n",
      "Epoch 116/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 29.4085 - val_loss: 72.9409\n",
      "Epoch 117/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 26.7687 - val_loss: 6.7634\n",
      "Epoch 118/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 26.5068 - val_loss: 14.0439\n",
      "Epoch 119/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 28.5029 - val_loss: 12.2062\n",
      "Epoch 120/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 23.6373 - val_loss: 21.0042\n",
      "Epoch 121/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 26.1388 - val_loss: 16.2414\n",
      "Epoch 122/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 23.8352 - val_loss: 13.5251\n",
      "Epoch 123/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 25.1663 - val_loss: 35.0751\n",
      "Epoch 124/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 25.6016 - val_loss: 20.1178\n",
      "Epoch 125/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 26.1083 - val_loss: 37.4460\n",
      "Epoch 126/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 24.7647 - val_loss: 12.8737\n",
      "Epoch 127/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 25.4983 - val_loss: 11.2087\n",
      "Epoch 128/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 24.3484 - val_loss: 86.3068\n",
      "Epoch 129/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 24.4036 - val_loss: 15.6779\n",
      "Epoch 130/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 26.3804 - val_loss: 9.0597\n",
      "Epoch 131/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 24.1206 - val_loss: 42.1689\n",
      "Epoch 132/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 23.0018 - val_loss: 26.6780\n",
      "Epoch 133/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 22.6776 - val_loss: 14.7567\n",
      "Epoch 134/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 24.5802 - val_loss: 34.1548\n",
      "Epoch 135/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 22.5964 - val_loss: 10.7864\n",
      "Epoch 136/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 23.9983 - val_loss: 8.3556\n",
      "Epoch 137/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 21.5766 - val_loss: 14.4158\n",
      "Epoch 138/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 24.4470 - val_loss: 7.6476\n",
      "Epoch 139/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 21.0861 - val_loss: 5.8100\n",
      "Epoch 140/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 22.5794 - val_loss: 16.0066\n",
      "Epoch 141/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 21.9412 - val_loss: 25.8186\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 71us/step - loss: 22.6123 - val_loss: 8.5060\n",
      "Epoch 143/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 22.9531 - val_loss: 28.4611\n",
      "Epoch 144/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 20.9347 - val_loss: 6.5104\n",
      "Epoch 145/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 23.1451 - val_loss: 6.9450\n",
      "Epoch 146/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 23.9988 - val_loss: 5.8474\n",
      "Epoch 147/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 22.1739 - val_loss: 10.2267\n",
      "Epoch 148/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 21.4382 - val_loss: 12.4742\n",
      "Epoch 149/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 21.8367 - val_loss: 28.5939\n",
      "Epoch 150/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 18.7784 - val_loss: 5.9022\n",
      "Epoch 151/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 22.4767 - val_loss: 8.9890\n",
      "Epoch 152/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 19.4763 - val_loss: 41.1643\n",
      "Epoch 153/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 21.1955 - val_loss: 51.4155\n",
      "Epoch 154/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 23.9790 - val_loss: 17.6260\n",
      "Epoch 155/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 20.5969 - val_loss: 20.8433\n",
      "Epoch 156/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 21.3603 - val_loss: 13.7667\n",
      "Epoch 157/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 19.4394 - val_loss: 15.1340\n",
      "Epoch 158/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 21.4173 - val_loss: 7.9071\n",
      "Epoch 159/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 21.6708 - val_loss: 10.5350\n",
      "Epoch 160/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 20.0674 - val_loss: 46.5773\n",
      "Epoch 161/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 20.3382 - val_loss: 7.3840\n",
      "Epoch 162/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 21.0186 - val_loss: 48.3317\n",
      "Epoch 163/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 19.7501 - val_loss: 47.1351\n",
      "Epoch 164/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 19.8450 - val_loss: 9.2813\n",
      "Epoch 165/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 20.6986 - val_loss: 9.4491\n",
      "Epoch 166/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 19.8495 - val_loss: 27.6089\n",
      "Epoch 167/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 18.6645 - val_loss: 7.3703\n",
      "Epoch 168/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 19.4262 - val_loss: 13.8097\n",
      "Epoch 169/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 19.9447 - val_loss: 45.8366\n",
      "Epoch 170/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 20.1286 - val_loss: 9.3432\n",
      "Epoch 171/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 16.2639 - val_loss: 29.4596\n",
      "Epoch 172/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 19.0977 - val_loss: 21.0282\n",
      "Epoch 173/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 18.8072 - val_loss: 7.9659\n",
      "Epoch 174/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 20.6177 - val_loss: 23.3780\n",
      "Epoch 175/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 17.4746 - val_loss: 13.5816\n",
      "Epoch 176/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 19.5576 - val_loss: 32.1145\n",
      "Epoch 177/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 21.6157 - val_loss: 21.0093\n",
      "Epoch 178/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 19.3770 - val_loss: 5.5210\n",
      "Epoch 179/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 18.2354 - val_loss: 50.4997\n",
      "Epoch 180/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 18.6467 - val_loss: 17.3574\n",
      "Epoch 181/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 19.2720 - val_loss: 7.6710\n",
      "Epoch 182/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 16.7926 - val_loss: 24.4565\n",
      "Epoch 183/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 18.5563 - val_loss: 45.0461\n",
      "Epoch 184/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 16.6338 - val_loss: 13.1620\n",
      "Epoch 185/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 17.7807 - val_loss: 31.9031\n",
      "Epoch 186/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 17.2773 - val_loss: 43.4205\n",
      "Epoch 187/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 18.1007 - val_loss: 68.2902\n",
      "Epoch 188/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 18.8948 - val_loss: 27.6982\n",
      "Epoch 189/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 16.8207 - val_loss: 15.6411\n",
      "Epoch 190/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 16.4539 - val_loss: 78.8170\n",
      "Epoch 191/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 18.6675 - val_loss: 8.4798\n",
      "Epoch 192/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 16.8242 - val_loss: 86.9308\n",
      "Epoch 193/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 18.4011 - val_loss: 27.0776\n",
      "Epoch 194/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 15.8926 - val_loss: 5.3980\n",
      "Epoch 195/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 17.4306 - val_loss: 13.8224\n",
      "Epoch 196/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 18.6048 - val_loss: 55.8205\n",
      "Epoch 197/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 17.7379 - val_loss: 7.7640\n",
      "Epoch 198/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 15.3723 - val_loss: 7.9927\n",
      "Epoch 199/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 18.0434 - val_loss: 3.8423\n",
      "Epoch 200/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 16.3608 - val_loss: 13.3955\n",
      "Epoch 201/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 16.2331 - val_loss: 11.1461\n",
      "Epoch 202/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 17.3354 - val_loss: 11.0711\n",
      "Epoch 203/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 16.3644 - val_loss: 39.5158\n",
      "Epoch 204/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 16.3638 - val_loss: 13.7082\n",
      "Epoch 205/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 16.0260 - val_loss: 6.0411\n",
      "Epoch 206/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 17.1072 - val_loss: 9.9008\n",
      "Epoch 207/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 16.3204 - val_loss: 15.9468\n",
      "Epoch 208/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 16.3724 - val_loss: 32.3123\n",
      "Epoch 209/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 15.9674 - val_loss: 14.4050\n",
      "Epoch 210/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 14.4664 - val_loss: 5.4017\n",
      "Epoch 211/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 14.9624 - val_loss: 5.8631\n",
      "Epoch 212/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 17.1043 - val_loss: 14.4322\n",
      "Epoch 213/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 14.4582 - val_loss: 6.4384\n",
      "Epoch 214/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 16.9950 - val_loss: 7.0698\n",
      "Epoch 215/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 15.4351 - val_loss: 4.4399\n",
      "Epoch 216/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 15.0020 - val_loss: 35.9983\n",
      "Epoch 217/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 16.3414 - val_loss: 8.7321\n",
      "Epoch 218/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 14.0066 - val_loss: 39.6310\n",
      "Epoch 219/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 16.2698 - val_loss: 58.8409\n",
      "Epoch 220/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 15.8249 - val_loss: 22.4740\n",
      "Epoch 221/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 15.7788 - val_loss: 11.2282\n",
      "Epoch 222/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 15.7745 - val_loss: 43.8479\n",
      "Epoch 223/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 15.6520 - val_loss: 8.9146\n",
      "Epoch 224/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 15.2477 - val_loss: 17.4780\n",
      "Epoch 225/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 15.2077 - val_loss: 6.5182\n",
      "Epoch 226/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 15.2825 - val_loss: 3.8815\n",
      "Epoch 227/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.4855 - val_loss: 3.9832\n",
      "Epoch 228/1000\n",
      "9081/9081 [==============================] - 1s 98us/step - loss: 15.0280 - val_loss: 6.5034\n",
      "Epoch 229/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 16.2226 - val_loss: 9.4739\n",
      "Epoch 230/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.5487 - val_loss: 6.5231\n",
      "Epoch 231/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 14.3161 - val_loss: 78.1995\n",
      "Epoch 232/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.3136 - val_loss: 81.6220\n",
      "Epoch 233/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.2041 - val_loss: 13.8567\n",
      "Epoch 234/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 14.6922 - val_loss: 8.1036\n",
      "Epoch 235/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 15.2612 - val_loss: 7.3366\n",
      "Epoch 236/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 14.6535 - val_loss: 5.5167\n",
      "Epoch 237/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.6126 - val_loss: 25.4793\n",
      "Epoch 238/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 15.7253 - val_loss: 5.6996\n",
      "Epoch 239/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.1819 - val_loss: 10.3565\n",
      "Epoch 240/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.9467 - val_loss: 4.2864\n",
      "Epoch 241/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 12.1645 - val_loss: 46.9360\n",
      "Epoch 242/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 13.8765 - val_loss: 15.8706\n",
      "Epoch 243/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 13.8281 - val_loss: 9.6473\n",
      "Epoch 244/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.1926 - val_loss: 17.9426\n",
      "Epoch 245/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 14.4857 - val_loss: 9.4270\n",
      "Epoch 246/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.6844 - val_loss: 12.2864\n",
      "Epoch 247/1000\n",
      "9081/9081 [==============================] - 1s 103us/step - loss: 13.1602 - val_loss: 72.9859\n",
      "Epoch 248/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 14.5785 - val_loss: 8.5276\n",
      "Epoch 249/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 13.4672 - val_loss: 5.1437\n",
      "Epoch 250/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 17.3811 - val_loss: 6.0210\n",
      "Epoch 251/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.5419 - val_loss: 4.4333\n",
      "Epoch 252/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 14.0126 - val_loss: 20.0771\n",
      "Epoch 253/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 14.3816 - val_loss: 5.8462\n",
      "Epoch 254/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 13.2844 - val_loss: 29.4386\n",
      "Epoch 255/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 13.7210 - val_loss: 25.6036\n",
      "Epoch 256/1000\n",
      "9081/9081 [==============================] - 1s 103us/step - loss: 14.5542 - val_loss: 9.7820\n",
      "Epoch 257/1000\n",
      "9081/9081 [==============================] - 1s 100us/step - loss: 13.4926 - val_loss: 6.7203\n",
      "Epoch 258/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.6974 - val_loss: 10.9394\n",
      "Epoch 259/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.1729 - val_loss: 7.3600\n",
      "Epoch 260/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 13.2780 - val_loss: 7.5364\n",
      "Epoch 261/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 13.2320 - val_loss: 7.5953\n",
      "Epoch 262/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 13.3832 - val_loss: 28.6495\n",
      "Epoch 263/1000\n",
      "9081/9081 [==============================] - 1s 84us/step - loss: 14.8226 - val_loss: 2.5879\n",
      "Epoch 264/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 13.8274 - val_loss: 7.3269\n",
      "Epoch 265/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 11.5114 - val_loss: 58.9922\n",
      "Epoch 266/1000\n",
      "9081/9081 [==============================] - 1s 62us/step - loss: 12.6511 - val_loss: 9.4824\n",
      "Epoch 267/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 14.2361 - val_loss: 9.7072\n",
      "Epoch 268/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.3502 - val_loss: 14.3133\n",
      "Epoch 269/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.7755 - val_loss: 20.4772\n",
      "Epoch 270/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 14.8686 - val_loss: 3.6795\n",
      "Epoch 271/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.6256 - val_loss: 15.0879\n",
      "Epoch 272/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 12.0729 - val_loss: 6.4685\n",
      "Epoch 273/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 11.7511 - val_loss: 10.6843\n",
      "Epoch 274/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.3568 - val_loss: 12.0705\n",
      "Epoch 275/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.0254 - val_loss: 39.7037\n",
      "Epoch 276/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 12.8648 - val_loss: 8.8515\n",
      "Epoch 277/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 12.7164 - val_loss: 16.9705\n",
      "Epoch 278/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.1044 - val_loss: 25.6725\n",
      "Epoch 279/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 13.0235 - val_loss: 13.0771\n",
      "Epoch 280/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.6983 - val_loss: 60.9939\n",
      "Epoch 281/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 13.4289 - val_loss: 3.1691\n",
      "Epoch 282/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.0446 - val_loss: 17.2718\n",
      "Epoch 283/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 13.5389 - val_loss: 6.7140\n",
      "Epoch 284/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.7567 - val_loss: 6.7047\n",
      "Epoch 285/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 12.9155 - val_loss: 20.8758\n",
      "Epoch 286/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 12.9441 - val_loss: 11.2999\n",
      "Epoch 287/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 13.3303 - val_loss: 3.8016\n",
      "Epoch 288/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.3379 - val_loss: 4.3925\n",
      "Epoch 289/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 13.2557 - val_loss: 12.7630\n",
      "Epoch 290/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.8083 - val_loss: 19.1796\n",
      "Epoch 291/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.2188 - val_loss: 10.2421\n",
      "Epoch 292/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.8651 - val_loss: 6.1018\n",
      "Epoch 293/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 12.4428 - val_loss: 17.2432\n",
      "Epoch 294/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 12.1252 - val_loss: 29.1498\n",
      "Epoch 295/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.5838 - val_loss: 17.0098\n",
      "Epoch 296/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 12.1140 - val_loss: 6.3169\n",
      "Epoch 297/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 11.8658 - val_loss: 7.5075\n",
      "Epoch 298/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.9769 - val_loss: 8.7460\n",
      "Epoch 299/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 12.4362 - val_loss: 4.8384\n",
      "Epoch 300/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.7740 - val_loss: 13.9875\n",
      "Epoch 301/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.3607 - val_loss: 18.0367\n",
      "Epoch 302/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 11.7793 - val_loss: 16.4847\n",
      "Epoch 303/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 11.3536 - val_loss: 7.7447\n",
      "Epoch 304/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.8429 - val_loss: 16.0109\n",
      "Epoch 305/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.8093 - val_loss: 15.9333\n",
      "Epoch 306/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.4621 - val_loss: 7.8491\n",
      "Epoch 307/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 13.3979 - val_loss: 5.5572\n",
      "Epoch 308/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.5486 - val_loss: 6.5992\n",
      "Epoch 309/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.5987 - val_loss: 12.8220\n",
      "Epoch 310/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 12.6307 - val_loss: 4.3064\n",
      "Epoch 311/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.8276 - val_loss: 24.0552\n",
      "Epoch 312/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.7787 - val_loss: 8.9638\n",
      "Epoch 313/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.3966 - val_loss: 16.6740\n",
      "Epoch 314/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.0052 - val_loss: 7.6809\n",
      "Epoch 315/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.2124 - val_loss: 25.0141\n",
      "Epoch 316/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.4286 - val_loss: 8.8608\n",
      "Epoch 317/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.5600 - val_loss: 23.2724\n",
      "Epoch 318/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.7052 - val_loss: 3.2946\n",
      "Epoch 319/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 11.0949 - val_loss: 26.0873\n",
      "Epoch 320/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 11.4083 - val_loss: 3.7714\n",
      "Epoch 321/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.8141 - val_loss: 10.7195\n",
      "Epoch 322/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.0529 - val_loss: 10.2604\n",
      "Epoch 323/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.9633 - val_loss: 22.1829\n",
      "Epoch 324/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.1786 - val_loss: 17.4895\n",
      "Epoch 325/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 12.0709 - val_loss: 4.6103\n",
      "Epoch 326/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.8780 - val_loss: 7.4899\n",
      "Epoch 327/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.6889 - val_loss: 5.0627\n",
      "Epoch 328/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.3462 - val_loss: 7.7380\n",
      "Epoch 329/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.8518 - val_loss: 6.0170\n",
      "Epoch 330/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.6612 - val_loss: 51.4570\n",
      "Epoch 331/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.2657 - val_loss: 13.0155\n",
      "Epoch 332/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.4618 - val_loss: 5.8813\n",
      "Epoch 333/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.0850 - val_loss: 38.1490\n",
      "Epoch 334/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.4926 - val_loss: 20.1987\n",
      "Epoch 335/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 11.9348 - val_loss: 27.5817\n",
      "Epoch 336/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.7018 - val_loss: 27.0177\n",
      "Epoch 337/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.2109 - val_loss: 8.7585\n",
      "Epoch 338/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 10.4689 - val_loss: 9.9027\n",
      "Epoch 339/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.3367 - val_loss: 10.2799\n",
      "Epoch 340/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 11.5601 - val_loss: 4.3327\n",
      "Epoch 341/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 9.8156 - val_loss: 18.4584\n",
      "Epoch 342/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 11.7295 - val_loss: 13.3044\n",
      "Epoch 343/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 11.7909 - val_loss: 14.2606\n",
      "Epoch 344/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 11.1346 - val_loss: 8.2125\n",
      "Epoch 345/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 11.1880 - val_loss: 7.2150\n",
      "Epoch 346/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.6066 - val_loss: 8.4777\n",
      "Epoch 347/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 12.3120 - val_loss: 10.8916\n",
      "Epoch 348/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 12.5956 - val_loss: 8.2392\n",
      "Epoch 349/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.4107 - val_loss: 10.8177\n",
      "Epoch 350/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.8974 - val_loss: 7.5322\n",
      "Epoch 351/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.7998 - val_loss: 12.8235\n",
      "Epoch 352/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 12.1168 - val_loss: 4.8703\n",
      "Epoch 353/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.1754 - val_loss: 8.2930\n",
      "Epoch 354/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.9015 - val_loss: 6.7800\n",
      "Epoch 355/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.4209 - val_loss: 12.7055\n",
      "Epoch 356/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.6182 - val_loss: 11.8577\n",
      "Epoch 357/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 10.1279 - val_loss: 15.6248\n",
      "Epoch 358/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.0394 - val_loss: 4.9227\n",
      "Epoch 359/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.0673 - val_loss: 6.6019\n",
      "Epoch 360/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.0672 - val_loss: 19.9489\n",
      "Epoch 361/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.9330 - val_loss: 7.2750\n",
      "Epoch 362/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.6118 - val_loss: 17.4337\n",
      "Epoch 363/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.5751 - val_loss: 7.6514\n",
      "Epoch 364/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 10.7403 - val_loss: 13.5338\n",
      "Epoch 365/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.1455 - val_loss: 10.2375\n",
      "Epoch 366/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 11.3359 - val_loss: 11.8691\n",
      "Epoch 367/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.2510 - val_loss: 12.7794\n",
      "Epoch 368/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 10.3862 - val_loss: 16.2515\n",
      "Epoch 369/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.1159 - val_loss: 4.7081\n",
      "Epoch 370/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.4234 - val_loss: 29.9397\n",
      "Epoch 371/1000\n",
      "9081/9081 [==============================] - 1s 92us/step - loss: 10.9114 - val_loss: 10.5280\n",
      "Epoch 372/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 10.9611 - val_loss: 10.4757\n",
      "Epoch 373/1000\n",
      "9081/9081 [==============================] - 1s 92us/step - loss: 9.9543 - val_loss: 3.3822\n",
      "Epoch 374/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 10.0039 - val_loss: 4.5952\n",
      "Epoch 375/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 8.7536 - val_loss: 8.5918\n",
      "Epoch 376/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 11.9146 - val_loss: 9.9382\n",
      "Epoch 377/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 9.3593 - val_loss: 8.2990\n",
      "Epoch 378/1000\n",
      "9081/9081 [==============================] - 1s 111us/step - loss: 9.7628 - val_loss: 5.0695\n",
      "Epoch 379/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 11.0527 - val_loss: 38.9682\n",
      "Epoch 380/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.0847 - val_loss: 27.7181\n",
      "Epoch 381/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.3972 - val_loss: 7.1954\n",
      "Epoch 382/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.8935 - val_loss: 5.5567\n",
      "Epoch 383/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.0804 - val_loss: 16.1339\n",
      "Epoch 384/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.4905 - val_loss: 8.4222\n",
      "Epoch 385/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.7355 - val_loss: 31.6673\n",
      "Epoch 386/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.5175 - val_loss: 8.8417\n",
      "Epoch 387/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.2129 - val_loss: 8.5530\n",
      "Epoch 388/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.4042 - val_loss: 11.4430\n",
      "Epoch 389/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.0695 - val_loss: 6.9475\n",
      "Epoch 390/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 11.1747 - val_loss: 9.6177\n",
      "Epoch 391/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.4595 - val_loss: 5.9455\n",
      "Epoch 392/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 10.0573 - val_loss: 7.0140\n",
      "Epoch 393/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.1141 - val_loss: 24.5544\n",
      "Epoch 394/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.2184 - val_loss: 7.4999\n",
      "Epoch 395/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.0213 - val_loss: 8.2573\n",
      "Epoch 396/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.3264 - val_loss: 7.7179\n",
      "Epoch 397/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 7.8308 - val_loss: 9.0429\n",
      "Epoch 398/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.7838 - val_loss: 20.1341\n",
      "Epoch 399/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.3551 - val_loss: 8.0135\n",
      "Epoch 400/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 8.9340 - val_loss: 5.2801\n",
      "Epoch 401/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.3492 - val_loss: 3.1307\n",
      "Epoch 402/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 8.8771 - val_loss: 34.4311\n",
      "Epoch 403/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.1314 - val_loss: 4.1610\n",
      "Epoch 404/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.2092 - val_loss: 11.6404\n",
      "Epoch 405/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.8428 - val_loss: 6.2955\n",
      "Epoch 406/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.3307 - val_loss: 26.6351\n",
      "Epoch 407/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.3959 - val_loss: 15.0907\n",
      "Epoch 408/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 7.7603 - val_loss: 27.4528\n",
      "Epoch 409/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 10.8275 - val_loss: 10.8881\n",
      "Epoch 410/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.2979 - val_loss: 5.5391\n",
      "Epoch 411/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.6115 - val_loss: 2.9599\n",
      "Epoch 412/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.5333 - val_loss: 2.2163\n",
      "Epoch 413/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.7595 - val_loss: 29.6316\n",
      "Epoch 414/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.1641 - val_loss: 10.0359\n",
      "Epoch 415/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.1864 - val_loss: 17.7001\n",
      "Epoch 416/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 10.5243 - val_loss: 5.2809\n",
      "Epoch 417/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.2941 - val_loss: 9.8052\n",
      "Epoch 418/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.2886 - val_loss: 17.2798\n",
      "Epoch 419/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.5502 - val_loss: 27.2253\n",
      "Epoch 420/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.8510 - val_loss: 6.9966\n",
      "Epoch 421/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.4907 - val_loss: 7.2669\n",
      "Epoch 422/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 9.7063 - val_loss: 22.4096\n",
      "Epoch 423/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.8190 - val_loss: 5.1448\n",
      "Epoch 424/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 11.2995 - val_loss: 7.2049\n",
      "Epoch 425/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 8.4414 - val_loss: 3.6819\n",
      "Epoch 426/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.5714 - val_loss: 16.8270\n",
      "Epoch 427/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.9380 - val_loss: 32.3244\n",
      "Epoch 428/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.3312 - val_loss: 7.1868\n",
      "Epoch 429/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.2662 - val_loss: 30.2255\n",
      "Epoch 430/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.6602 - val_loss: 23.7181\n",
      "Epoch 431/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.9270 - val_loss: 6.0572\n",
      "Epoch 432/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.8101 - val_loss: 7.6464\n",
      "Epoch 433/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 10.3867 - val_loss: 5.9608\n",
      "Epoch 434/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.3406 - val_loss: 5.2419\n",
      "Epoch 435/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.3463 - val_loss: 21.9437\n",
      "Epoch 436/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.3488 - val_loss: 8.3225\n",
      "Epoch 437/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.6991 - val_loss: 5.5490\n",
      "Epoch 438/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 8.4710 - val_loss: 3.4062\n",
      "Epoch 439/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.2022 - val_loss: 17.7650\n",
      "Epoch 440/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.4396 - val_loss: 44.7558\n",
      "Epoch 441/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.7702 - val_loss: 6.4300\n",
      "Epoch 442/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.8973 - val_loss: 18.5111\n",
      "Epoch 443/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 69us/step - loss: 10.2116 - val_loss: 5.6634\n",
      "Epoch 444/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 8.0417 - val_loss: 11.9514\n",
      "Epoch 445/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 8.6069 - val_loss: 2.8848\n",
      "Epoch 446/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 9.2376 - val_loss: 10.8051\n",
      "Epoch 447/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 8.5198 - val_loss: 5.5504\n",
      "Epoch 448/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 8.2606 - val_loss: 7.3136\n",
      "Epoch 449/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 9.0291 - val_loss: 20.9600\n",
      "Epoch 450/1000\n",
      "9081/9081 [==============================] - 1s 91us/step - loss: 9.1538 - val_loss: 18.7826\n",
      "Epoch 451/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 9.8858 - val_loss: 3.2882\n",
      "Epoch 452/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 8.0506 - val_loss: 5.8208\n",
      "Epoch 453/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 7.4371 - val_loss: 16.6584\n",
      "Epoch 454/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 8.6099 - val_loss: 12.5099\n",
      "Epoch 455/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 8.8794 - val_loss: 7.3252\n",
      "Epoch 456/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 9.3029 - val_loss: 5.7241\n",
      "Epoch 457/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 8.3379 - val_loss: 47.0794\n",
      "Epoch 458/1000\n",
      "9081/9081 [==============================] - 1s 94us/step - loss: 8.9790 - val_loss: 17.0092\n",
      "Epoch 459/1000\n",
      "9081/9081 [==============================] - 1s 83us/step - loss: 9.3024 - val_loss: 8.2950\n",
      "Epoch 460/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 7.5586 - val_loss: 12.0257\n",
      "Epoch 461/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 8.7034 - val_loss: 65.2853\n",
      "Epoch 462/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.4061 - val_loss: 11.1985\n",
      "Epoch 463/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.7091 - val_loss: 7.8540\n",
      "Epoch 464/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 9.4727 - val_loss: 4.8715\n",
      "Epoch 465/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.9609 - val_loss: 3.8504\n",
      "Epoch 466/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.4432 - val_loss: 9.3431\n",
      "Epoch 467/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.9965 - val_loss: 8.1700\n",
      "Epoch 468/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.3299 - val_loss: 7.8014\n",
      "Epoch 469/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 7.9033 - val_loss: 2.5709\n",
      "Epoch 470/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.0537 - val_loss: 6.4398\n",
      "Epoch 471/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.6524 - val_loss: 10.0211\n",
      "Epoch 472/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.2666 - val_loss: 9.6923\n",
      "Epoch 473/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.1240 - val_loss: 3.3488\n",
      "Epoch 474/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 7.3035 - val_loss: 9.4460\n",
      "Epoch 475/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 9.0843 - val_loss: 9.9991\n",
      "Epoch 476/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.7029 - val_loss: 6.2674\n",
      "Epoch 477/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 9.4818 - val_loss: 5.0756\n",
      "Epoch 478/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.1397 - val_loss: 4.8845\n",
      "Epoch 479/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 7.5178 - val_loss: 10.0597\n",
      "Epoch 480/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 7.7768 - val_loss: 19.5166\n",
      "Epoch 481/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 8.2562 - val_loss: 5.6764\n",
      "Epoch 482/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 7.0579 - val_loss: 3.1212\n",
      "Epoch 483/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 8.8440 - val_loss: 4.9599\n",
      "Epoch 484/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 9.1934 - val_loss: 7.5242\n",
      "Epoch 485/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.9596 - val_loss: 27.4706\n",
      "Epoch 486/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 8.7726 - val_loss: 9.2470\n",
      "Epoch 487/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.3338 - val_loss: 5.8587\n",
      "Epoch 488/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.3230 - val_loss: 8.1168\n",
      "Epoch 489/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.0561 - val_loss: 11.0997\n",
      "Epoch 490/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.9197 - val_loss: 8.2593\n",
      "Epoch 491/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 7.7621 - val_loss: 2.6694\n",
      "Epoch 492/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 9.0625 - val_loss: 17.1510\n",
      "Epoch 493/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 8.1950 - val_loss: 8.7247\n",
      "Epoch 494/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 8.5531 - val_loss: 7.5979\n",
      "Epoch 495/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.9874 - val_loss: 40.3325\n",
      "Epoch 496/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 8.2335 - val_loss: 21.1332\n",
      "Epoch 497/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 8.4720 - val_loss: 12.7279\n",
      "Epoch 498/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.9347 - val_loss: 23.3370\n",
      "Epoch 499/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 9.1421 - val_loss: 6.0812\n",
      "Epoch 500/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 9.3577 - val_loss: 3.5963\n",
      "Epoch 501/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 8.7987 - val_loss: 10.7320\n",
      "Epoch 502/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 7.1078 - val_loss: 11.5558\n",
      "Epoch 503/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 10.1392 - val_loss: 6.7949\n",
      "Epoch 504/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.9690 - val_loss: 6.4591\n",
      "Epoch 505/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.9396 - val_loss: 2.9063\n",
      "Epoch 506/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 8.6732 - val_loss: 2.9631\n",
      "Epoch 507/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.5800 - val_loss: 2.9717\n",
      "Epoch 508/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 7.2292 - val_loss: 27.4663\n",
      "Epoch 509/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 8.4464 - val_loss: 14.9534\n",
      "Epoch 510/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 8.0355 - val_loss: 6.9377\n",
      "Epoch 511/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 8.0777 - val_loss: 9.0009\n",
      "Epoch 512/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2389 - val_loss: 2.7934\n",
      "Epoch 513/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 9.9577 - val_loss: 5.6800\n",
      "Epoch 514/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.5373 - val_loss: 17.5199\n",
      "Epoch 515/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 8.5075 - val_loss: 14.2473\n",
      "Epoch 516/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 8.4260 - val_loss: 9.6580\n",
      "Epoch 517/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 7.9726 - val_loss: 12.6526\n",
      "Epoch 518/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 8.0299 - val_loss: 10.8018\n",
      "Epoch 519/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.8802 - val_loss: 37.1375\n",
      "Epoch 520/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 7.9647 - val_loss: 7.1839\n",
      "Epoch 521/1000\n",
      "9081/9081 [==============================] - 1s 95us/step - loss: 7.8297 - val_loss: 8.4964\n",
      "Epoch 522/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 9.6556 - val_loss: 6.9407\n",
      "Epoch 523/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.8013 - val_loss: 14.5946\n",
      "Epoch 524/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.4991 - val_loss: 40.8688\n",
      "Epoch 525/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.3452 - val_loss: 25.8121\n",
      "Epoch 526/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2924 - val_loss: 7.6938\n",
      "Epoch 527/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 8.8030 - val_loss: 2.7053\n",
      "Epoch 528/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.2769 - val_loss: 7.7137\n",
      "Epoch 529/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 7.7548 - val_loss: 7.2939\n",
      "Epoch 530/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.9136 - val_loss: 2.4253\n",
      "Epoch 531/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 8.2428 - val_loss: 8.1246\n",
      "Epoch 532/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 8.8473 - val_loss: 2.6285\n",
      "Epoch 533/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.1944 - val_loss: 28.6355\n",
      "Epoch 534/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.8354 - val_loss: 4.3490\n",
      "Epoch 535/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 9.0592 - val_loss: 5.5221\n",
      "Epoch 536/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 5.9272 - val_loss: 5.7507\n",
      "Epoch 537/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 7.9988 - val_loss: 2.5469\n",
      "Epoch 538/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 8.0308 - val_loss: 6.5983\n",
      "Epoch 539/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.9816 - val_loss: 28.0494\n",
      "Epoch 540/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 8.4779 - val_loss: 15.1210\n",
      "Epoch 541/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 7.7130 - val_loss: 6.8913\n",
      "Epoch 542/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.4390 - val_loss: 6.0925\n",
      "Epoch 543/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.7999 - val_loss: 16.0871\n",
      "Epoch 544/1000\n",
      "9081/9081 [==============================] - 1s 103us/step - loss: 8.6649 - val_loss: 14.2770\n",
      "Epoch 545/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 8.0753 - val_loss: 22.8294\n",
      "Epoch 546/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.7940 - val_loss: 8.7603\n",
      "Epoch 547/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.1567 - val_loss: 12.4515\n",
      "Epoch 548/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.7745 - val_loss: 26.6868\n",
      "Epoch 549/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 7.6203 - val_loss: 10.5006\n",
      "Epoch 550/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.5476 - val_loss: 7.6414\n",
      "Epoch 551/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.9726 - val_loss: 16.0105\n",
      "Epoch 552/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 6.9646 - val_loss: 9.7413\n",
      "Epoch 553/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 7.3323 - val_loss: 4.1302\n",
      "Epoch 554/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 7.3303 - val_loss: 4.8776\n",
      "Epoch 555/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 8.7196 - val_loss: 4.6413\n",
      "Epoch 556/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 6.6343 - val_loss: 7.0881\n",
      "Epoch 557/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 7.6753 - val_loss: 2.4415\n",
      "Epoch 558/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 8.2274 - val_loss: 6.8831\n",
      "Epoch 559/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 8.2293 - val_loss: 14.1556\n",
      "Epoch 560/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.8215 - val_loss: 3.2671\n",
      "Epoch 561/1000\n",
      "9081/9081 [==============================] - 1s 85us/step - loss: 6.9588 - val_loss: 3.6040\n",
      "Epoch 562/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 8.6466 - val_loss: 5.2757\n",
      "Epoch 563/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 7.2153 - val_loss: 4.9059\n",
      "Epoch 564/1000\n",
      "9081/9081 [==============================] - 1s 64us/step - loss: 7.7172 - val_loss: 7.6537\n",
      "Epoch 565/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 7.5036 - val_loss: 7.8666\n",
      "Epoch 566/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.3496 - val_loss: 3.5643\n",
      "Epoch 567/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.1129 - val_loss: 6.5136\n",
      "Epoch 568/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.5372 - val_loss: 7.9531\n",
      "Epoch 569/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.1745 - val_loss: 6.7729\n",
      "Epoch 570/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.8861 - val_loss: 6.8902\n",
      "Epoch 571/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 8.1214 - val_loss: 4.9492\n",
      "Epoch 572/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 7.6297 - val_loss: 5.7589\n",
      "Epoch 573/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.1082 - val_loss: 8.1398\n",
      "Epoch 574/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 7.2614 - val_loss: 33.2126\n",
      "Epoch 575/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.6387 - val_loss: 2.6313\n",
      "Epoch 576/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 6.4269 - val_loss: 7.2518\n",
      "Epoch 577/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.1190 - val_loss: 2.9534\n",
      "Epoch 578/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 7.0464 - val_loss: 6.7964\n",
      "Epoch 579/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 8.1337 - val_loss: 2.5428\n",
      "Epoch 580/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.3815 - val_loss: 7.5975\n",
      "Epoch 581/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 7.7306 - val_loss: 9.2263\n",
      "Epoch 582/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 7.9144 - val_loss: 20.0708\n",
      "Epoch 583/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.9124 - val_loss: 8.5829\n",
      "Epoch 584/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2034 - val_loss: 8.4460\n",
      "Epoch 585/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 7.6005 - val_loss: 4.8240\n",
      "Epoch 586/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2316 - val_loss: 15.3572\n",
      "Epoch 587/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 8.2827 - val_loss: 6.7581\n",
      "Epoch 588/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.1624 - val_loss: 2.4270\n",
      "Epoch 589/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.5494 - val_loss: 9.3492\n",
      "Epoch 590/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 7.7331 - val_loss: 7.0491\n",
      "Epoch 591/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2689 - val_loss: 21.2028\n",
      "Epoch 592/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.2187 - val_loss: 3.0586\n",
      "Epoch 593/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 6.3387 - val_loss: 19.1979\n",
      "Epoch 594/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 7.9268 - val_loss: 7.1061\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 96us/step - loss: 7.8515 - val_loss: 6.6654\n",
      "Epoch 596/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2202 - val_loss: 5.8474\n",
      "Epoch 597/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.6305 - val_loss: 4.5638\n",
      "Epoch 598/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.7179 - val_loss: 8.1732\n",
      "Epoch 599/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.5156 - val_loss: 6.7837\n",
      "Epoch 600/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.0621 - val_loss: 8.3937\n",
      "Epoch 601/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 6.9884 - val_loss: 4.9507\n",
      "Epoch 602/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 7.9701 - val_loss: 7.0225\n",
      "Epoch 603/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.4754 - val_loss: 18.4273\n",
      "Epoch 604/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 7.7387 - val_loss: 6.6697\n",
      "Epoch 605/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.9673 - val_loss: 7.0773\n",
      "Epoch 606/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.5475 - val_loss: 3.1152\n",
      "Epoch 607/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.8625 - val_loss: 6.1906\n",
      "Epoch 608/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 6.2843 - val_loss: 2.4238\n",
      "Epoch 609/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 7.1730 - val_loss: 2.2249\n",
      "Epoch 610/1000\n",
      "9081/9081 [==============================] - 1s 94us/step - loss: 7.5033 - val_loss: 3.7246\n",
      "Epoch 611/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 6.9404 - val_loss: 11.7154\n",
      "Epoch 612/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.7857 - val_loss: 15.1633\n",
      "Epoch 613/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 6.3251 - val_loss: 13.3959\n",
      "Epoch 614/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.2135 - val_loss: 8.0318\n",
      "Epoch 615/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.5795 - val_loss: 7.3899\n",
      "Epoch 616/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 6.8999 - val_loss: 4.6751\n",
      "Epoch 617/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 7.3646 - val_loss: 6.2831\n",
      "Epoch 618/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.7794 - val_loss: 14.8548\n",
      "Epoch 619/1000\n",
      "9081/9081 [==============================] - 1s 102us/step - loss: 7.7263 - val_loss: 9.9497\n",
      "Epoch 620/1000\n",
      "9081/9081 [==============================] - 1s 65us/step - loss: 6.1120 - val_loss: 4.8933\n",
      "Epoch 621/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.0345 - val_loss: 3.2087\n",
      "Epoch 622/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.7913 - val_loss: 4.1037\n",
      "Epoch 623/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.8393 - val_loss: 7.0456\n",
      "Epoch 624/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.4048 - val_loss: 11.4652\n",
      "Epoch 625/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.5885 - val_loss: 32.1894\n",
      "Epoch 626/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 7.0928 - val_loss: 10.2597\n",
      "Epoch 627/1000\n",
      "9081/9081 [==============================] - 1s 65us/step - loss: 6.3683 - val_loss: 12.2794\n",
      "Epoch 628/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 7.4847 - val_loss: 22.6232\n",
      "Epoch 629/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.1638 - val_loss: 18.4863\n",
      "Epoch 630/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 6.6019 - val_loss: 12.3027\n",
      "Epoch 631/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.8800 - val_loss: 11.7284\n",
      "Epoch 632/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.2643 - val_loss: 4.0553\n",
      "Epoch 633/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.5962 - val_loss: 7.8428\n",
      "Epoch 634/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.8270 - val_loss: 5.8157\n",
      "Epoch 635/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.1927 - val_loss: 4.4835\n",
      "Epoch 636/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 6.0749 - val_loss: 7.1248\n",
      "Epoch 637/1000\n",
      "9081/9081 [==============================] - 1s 65us/step - loss: 7.4914 - val_loss: 5.1584\n",
      "Epoch 638/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 7.4604 - val_loss: 8.3927\n",
      "Epoch 639/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.2530 - val_loss: 10.6469\n",
      "Epoch 640/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.1280 - val_loss: 7.2288\n",
      "Epoch 641/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.7035 - val_loss: 21.1494\n",
      "Epoch 642/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.2277 - val_loss: 7.1472\n",
      "Epoch 643/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.5310 - val_loss: 9.5748\n",
      "Epoch 644/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.0524 - val_loss: 7.3107\n",
      "Epoch 645/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.0860 - val_loss: 7.6110\n",
      "Epoch 646/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.8244 - val_loss: 10.6017\n",
      "Epoch 647/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.6391 - val_loss: 13.0385\n",
      "Epoch 648/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.7065 - val_loss: 13.7288\n",
      "Epoch 649/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.6750 - val_loss: 8.8160\n",
      "Epoch 650/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.3516 - val_loss: 18.1938\n",
      "Epoch 651/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 8.4158 - val_loss: 4.0540\n",
      "Epoch 652/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.5935 - val_loss: 6.6854\n",
      "Epoch 653/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.8511 - val_loss: 7.3769\n",
      "Epoch 654/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.7618 - val_loss: 28.1073\n",
      "Epoch 655/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 7.0949 - val_loss: 4.7951\n",
      "Epoch 656/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.7879 - val_loss: 2.5304\n",
      "Epoch 657/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.6316 - val_loss: 6.9745\n",
      "Epoch 658/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.6959 - val_loss: 5.2985\n",
      "Epoch 659/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.0891 - val_loss: 4.1476\n",
      "Epoch 660/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 5.9379 - val_loss: 5.2205\n",
      "Epoch 661/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.3291 - val_loss: 8.4761\n",
      "Epoch 662/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.4143 - val_loss: 6.6169\n",
      "Epoch 663/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.0225 - val_loss: 12.7075\n",
      "Epoch 664/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.5051 - val_loss: 7.1965\n",
      "Epoch 665/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.3327 - val_loss: 8.5061\n",
      "Epoch 666/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 7.7403 - val_loss: 8.1505\n",
      "Epoch 667/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 6.5207 - val_loss: 42.1574\n",
      "Epoch 668/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.5396 - val_loss: 7.4878\n",
      "Epoch 669/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.8083 - val_loss: 8.5432\n",
      "Epoch 670/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.4076 - val_loss: 5.6934\n",
      "Epoch 671/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 94us/step - loss: 6.1454 - val_loss: 4.8037\n",
      "Epoch 672/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 6.7022 - val_loss: 5.2402\n",
      "Epoch 673/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.3031 - val_loss: 5.7415\n",
      "Epoch 674/1000\n",
      "9081/9081 [==============================] - 1s 85us/step - loss: 6.7955 - val_loss: 9.3044\n",
      "Epoch 675/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 6.7937 - val_loss: 7.6090\n",
      "Epoch 676/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.1446 - val_loss: 31.7013\n",
      "Epoch 677/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.5434 - val_loss: 9.2293\n",
      "Epoch 678/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.8706 - val_loss: 6.6299\n",
      "Epoch 679/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.5565 - val_loss: 11.8000\n",
      "Epoch 680/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.7529 - val_loss: 10.8976\n",
      "Epoch 681/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 7.2181 - val_loss: 6.3419\n",
      "Epoch 682/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.8316 - val_loss: 2.4308\n",
      "Epoch 683/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.3912 - val_loss: 14.4168\n",
      "Epoch 684/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.7486 - val_loss: 8.6280\n",
      "Epoch 685/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.5517 - val_loss: 19.4529\n",
      "Epoch 686/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.9483 - val_loss: 7.3760\n",
      "Epoch 687/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.2250 - val_loss: 4.7930\n",
      "Epoch 688/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.5489 - val_loss: 7.3639\n",
      "Epoch 689/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.8951 - val_loss: 25.3710\n",
      "Epoch 690/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.3766 - val_loss: 8.0941\n",
      "Epoch 691/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.7047 - val_loss: 7.5749\n",
      "Epoch 692/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.3345 - val_loss: 15.2615\n",
      "Epoch 693/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.6136 - val_loss: 6.5543\n",
      "Epoch 694/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.5467 - val_loss: 4.7723\n",
      "Epoch 695/1000\n",
      "9081/9081 [==============================] - 1s 94us/step - loss: 6.8028 - val_loss: 11.2003\n",
      "Epoch 696/1000\n",
      "9081/9081 [==============================] - 1s 101us/step - loss: 5.3144 - val_loss: 10.6242\n",
      "Epoch 697/1000\n",
      "9081/9081 [==============================] - 1s 64us/step - loss: 7.3831 - val_loss: 8.6045\n",
      "Epoch 698/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 5.3773 - val_loss: 16.3607\n",
      "Epoch 699/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 6.6710 - val_loss: 33.7350\n",
      "Epoch 700/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.6093 - val_loss: 8.2283\n",
      "Epoch 701/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 5.8530 - val_loss: 8.9594\n",
      "Epoch 702/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.5525 - val_loss: 5.4589\n",
      "Epoch 703/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.9324 - val_loss: 6.2873\n",
      "Epoch 704/1000\n",
      "9081/9081 [==============================] - 1s 64us/step - loss: 6.0088 - val_loss: 7.3792\n",
      "Epoch 705/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 6.5618 - val_loss: 4.2786\n",
      "Epoch 706/1000\n",
      "9081/9081 [==============================] - 1s 63us/step - loss: 6.7335 - val_loss: 17.0035\n",
      "Epoch 707/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 6.6769 - val_loss: 9.0781\n",
      "Epoch 708/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.1826 - val_loss: 6.4847\n",
      "Epoch 709/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.7308 - val_loss: 11.3345\n",
      "Epoch 710/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 6.0784 - val_loss: 9.8295\n",
      "Epoch 711/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 5.7669 - val_loss: 6.0912\n",
      "Epoch 712/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.7546 - val_loss: 7.6928\n",
      "Epoch 713/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 6.1705 - val_loss: 2.2066\n",
      "Epoch 714/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 6.5229 - val_loss: 7.7547\n",
      "Epoch 715/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.7901 - val_loss: 7.5355\n",
      "Epoch 716/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 7.1844 - val_loss: 13.2299\n",
      "Epoch 717/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.5190 - val_loss: 6.0634\n",
      "Epoch 718/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.5834 - val_loss: 8.4917\n",
      "Epoch 719/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.3118 - val_loss: 9.2300\n",
      "Epoch 720/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.2202 - val_loss: 3.6667\n",
      "Epoch 721/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.9575 - val_loss: 7.5685\n",
      "Epoch 722/1000\n",
      "9081/9081 [==============================] - 1s 98us/step - loss: 5.3417 - val_loss: 6.9343\n",
      "Epoch 723/1000\n",
      "9081/9081 [==============================] - 1s 94us/step - loss: 6.4810 - val_loss: 18.0388\n",
      "Epoch 724/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.2350 - val_loss: 4.6139\n",
      "Epoch 725/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.0381 - val_loss: 2.5276\n",
      "Epoch 726/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.1776 - val_loss: 6.0947\n",
      "Epoch 727/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 7.0488 - val_loss: 3.2019\n",
      "Epoch 728/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 4.9882 - val_loss: 2.5727\n",
      "Epoch 729/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.2625 - val_loss: 7.1391\n",
      "Epoch 730/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.3268 - val_loss: 5.2626\n",
      "Epoch 731/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.8868 - val_loss: 6.9819\n",
      "Epoch 732/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.7045 - val_loss: 7.1572\n",
      "Epoch 733/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.6317 - val_loss: 6.1533\n",
      "Epoch 734/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.9097 - val_loss: 7.6214\n",
      "Epoch 735/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 7.0602 - val_loss: 5.0224\n",
      "Epoch 736/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.2905 - val_loss: 5.6638\n",
      "Epoch 737/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.2252 - val_loss: 5.7837\n",
      "Epoch 738/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.6861 - val_loss: 7.6157\n",
      "Epoch 739/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.9135 - val_loss: 10.0532\n",
      "Epoch 740/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.6042 - val_loss: 3.2419\n",
      "Epoch 741/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.3343 - val_loss: 5.8622\n",
      "Epoch 742/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.5196 - val_loss: 4.9314\n",
      "Epoch 743/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 6.3194 - val_loss: 3.3393\n",
      "Epoch 744/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.6480 - val_loss: 8.5496\n",
      "Epoch 745/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.8527 - val_loss: 7.1540\n",
      "Epoch 746/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.3550 - val_loss: 29.8502\n",
      "Epoch 747/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.1949 - val_loss: 7.3278\n",
      "Epoch 748/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.2754 - val_loss: 7.0510\n",
      "Epoch 749/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 5.6565 - val_loss: 9.1138\n",
      "Epoch 750/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.9142 - val_loss: 9.0012\n",
      "Epoch 751/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 4.6889 - val_loss: 9.9639\n",
      "Epoch 752/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.8759 - val_loss: 8.3260\n",
      "Epoch 753/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 5.5322 - val_loss: 7.2332\n",
      "Epoch 754/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 5.7298 - val_loss: 5.1243\n",
      "Epoch 755/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 6.4406 - val_loss: 11.4345\n",
      "Epoch 756/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 7.2101 - val_loss: 42.7401\n",
      "Epoch 757/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.3524 - val_loss: 6.6293\n",
      "Epoch 758/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 5.3940 - val_loss: 6.5793\n",
      "Epoch 759/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 5.4393 - val_loss: 6.7791\n",
      "Epoch 760/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.8865 - val_loss: 3.8761\n",
      "Epoch 761/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.6802 - val_loss: 14.7898\n",
      "Epoch 762/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.0117 - val_loss: 8.9588\n",
      "Epoch 763/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.5604 - val_loss: 9.1749\n",
      "Epoch 764/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.8343 - val_loss: 9.9879\n",
      "Epoch 765/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.4366 - val_loss: 4.9016\n",
      "Epoch 766/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.5862 - val_loss: 7.3837\n",
      "Epoch 767/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.7129 - val_loss: 8.9633\n",
      "Epoch 768/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.2415 - val_loss: 10.0871\n",
      "Epoch 769/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 6.6265 - val_loss: 2.8077\n",
      "Epoch 770/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 5.4723 - val_loss: 7.2279\n",
      "Epoch 771/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 6.1836 - val_loss: 8.2578\n",
      "Epoch 772/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 5.3402 - val_loss: 5.6898\n",
      "Epoch 773/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 6.0069 - val_loss: 6.8584\n",
      "Epoch 774/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 5.1703 - val_loss: 7.4904\n",
      "Epoch 775/1000\n",
      "9081/9081 [==============================] - 1s 85us/step - loss: 5.4046 - val_loss: 8.4500\n",
      "Epoch 776/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 5.6452 - val_loss: 10.6166\n",
      "Epoch 777/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.5021 - val_loss: 4.9654\n",
      "Epoch 778/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.3391 - val_loss: 7.1564\n",
      "Epoch 779/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.0549 - val_loss: 10.5773\n",
      "Epoch 780/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.3071 - val_loss: 6.5403\n",
      "Epoch 781/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 5.7916 - val_loss: 2.4540\n",
      "Epoch 782/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.4275 - val_loss: 20.9366\n",
      "Epoch 783/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 5.1977 - val_loss: 2.5403\n",
      "Epoch 784/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 4.9556 - val_loss: 9.8549\n",
      "Epoch 785/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.8955 - val_loss: 9.2283\n",
      "Epoch 786/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.2502 - val_loss: 5.7021\n",
      "Epoch 787/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.5161 - val_loss: 6.7841\n",
      "Epoch 788/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.9540 - val_loss: 5.2711\n",
      "Epoch 789/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 6.2127 - val_loss: 16.8133\n",
      "Epoch 790/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.8355 - val_loss: 13.7859\n",
      "Epoch 791/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.7038 - val_loss: 7.3543\n",
      "Epoch 792/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.7584 - val_loss: 4.5116\n",
      "Epoch 793/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 5.4273 - val_loss: 6.7223\n",
      "Epoch 794/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 6.8084 - val_loss: 13.1921\n",
      "Epoch 795/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 4.5624 - val_loss: 15.4313\n",
      "Epoch 796/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.7884 - val_loss: 5.1129\n",
      "Epoch 797/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 5.2496 - val_loss: 28.6197\n",
      "Epoch 798/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 5.9266 - val_loss: 9.9954\n",
      "Epoch 799/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.4001 - val_loss: 3.1669\n",
      "Epoch 800/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 6.2073 - val_loss: 2.9137\n",
      "Epoch 801/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 6.2048 - val_loss: 29.6022\n",
      "Epoch 802/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 5.8926 - val_loss: 7.3426\n",
      "Epoch 803/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 5.8922 - val_loss: 2.8489\n",
      "Epoch 804/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 6.0913 - val_loss: 7.1683\n",
      "Epoch 805/1000\n",
      "9081/9081 [==============================] - 1s 106us/step - loss: 6.8113 - val_loss: 6.7686\n",
      "Epoch 806/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.7862 - val_loss: 10.5432\n",
      "Epoch 807/1000\n",
      "9081/9081 [==============================] - 1s 107us/step - loss: 5.9724 - val_loss: 10.0775\n",
      "Epoch 808/1000\n",
      "9081/9081 [==============================] - 1s 99us/step - loss: 5.1623 - val_loss: 8.8846\n",
      "Epoch 809/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.0734 - val_loss: 3.7521\n",
      "Epoch 810/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.8908 - val_loss: 6.0279\n",
      "Epoch 811/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.3745 - val_loss: 17.8172\n",
      "Epoch 812/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 6.0853 - val_loss: 3.1322\n",
      "Epoch 813/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 5.8313 - val_loss: 14.4215\n",
      "Epoch 814/1000\n",
      "9081/9081 [==============================] - 1s 88us/step - loss: 6.5324 - val_loss: 6.0119\n",
      "Epoch 815/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.8948 - val_loss: 8.8508\n",
      "Epoch 816/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 5.1561 - val_loss: 5.7771\n",
      "Epoch 817/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.5906 - val_loss: 3.0232\n",
      "Epoch 818/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 4.7841 - val_loss: 7.8655\n",
      "Epoch 819/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 5.0983 - val_loss: 7.5066\n",
      "Epoch 820/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.6918 - val_loss: 2.4278\n",
      "Epoch 821/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.3048 - val_loss: 3.1804\n",
      "Epoch 822/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.8740 - val_loss: 18.3007\n",
      "Epoch 823/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.3751 - val_loss: 8.6957\n",
      "Epoch 824/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.6021 - val_loss: 10.7645\n",
      "Epoch 825/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 4.7738 - val_loss: 6.1495\n",
      "Epoch 826/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 5.7161 - val_loss: 5.7864\n",
      "Epoch 827/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.1317 - val_loss: 6.5435\n",
      "Epoch 828/1000\n",
      "9081/9081 [==============================] - 1s 66us/step - loss: 6.2934 - val_loss: 6.8963\n",
      "Epoch 829/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 6.2156 - val_loss: 11.4099\n",
      "Epoch 830/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.9042 - val_loss: 3.4276\n",
      "Epoch 831/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.9120 - val_loss: 6.7005\n",
      "Epoch 832/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.8117 - val_loss: 8.7194\n",
      "Epoch 833/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.6353 - val_loss: 12.5746\n",
      "Epoch 834/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.4289 - val_loss: 5.0051\n",
      "Epoch 835/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.9753 - val_loss: 7.1478\n",
      "Epoch 836/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.2615 - val_loss: 3.1875\n",
      "Epoch 837/1000\n",
      "9081/9081 [==============================] - 1s 109us/step - loss: 5.8926 - val_loss: 7.2287\n",
      "Epoch 838/1000\n",
      "9081/9081 [==============================] - 1s 108us/step - loss: 5.5687 - val_loss: 4.1696\n",
      "Epoch 839/1000\n",
      "9081/9081 [==============================] - 1s 104us/step - loss: 6.1268 - val_loss: 7.3999\n",
      "Epoch 840/1000\n",
      "9081/9081 [==============================] - 1s 94us/step - loss: 5.6102 - val_loss: 5.9760\n",
      "Epoch 841/1000\n",
      "9081/9081 [==============================] - 1s 95us/step - loss: 4.8180 - val_loss: 2.4040\n",
      "Epoch 842/1000\n",
      "9081/9081 [==============================] - 1s 96us/step - loss: 6.1813 - val_loss: 13.6934\n",
      "Epoch 843/1000\n",
      "9081/9081 [==============================] - 1s 97us/step - loss: 5.9243 - val_loss: 7.1343\n",
      "Epoch 844/1000\n",
      "9081/9081 [==============================] - 1s 111us/step - loss: 5.5965 - val_loss: 5.6831\n",
      "Epoch 845/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 6.4542 - val_loss: 8.8231\n",
      "Epoch 846/1000\n",
      "9081/9081 [==============================] - 1s 123us/step - loss: 5.4366 - val_loss: 38.4391\n",
      "Epoch 847/1000\n",
      "9081/9081 [==============================] - 1s 157us/step - loss: 6.7239 - val_loss: 7.3954\n",
      "Epoch 848/1000\n",
      "9081/9081 [==============================] - 1s 137us/step - loss: 5.0730 - val_loss: 3.4170\n",
      "Epoch 849/1000\n",
      "9081/9081 [==============================] - 1s 130us/step - loss: 5.6380 - val_loss: 6.9340\n",
      "Epoch 850/1000\n",
      "9081/9081 [==============================] - 1s 123us/step - loss: 5.5543 - val_loss: 5.0597\n",
      "Epoch 851/1000\n",
      "9081/9081 [==============================] - 1s 140us/step - loss: 5.4236 - val_loss: 11.3640\n",
      "Epoch 852/1000\n",
      "9081/9081 [==============================] - 1s 160us/step - loss: 5.7338 - val_loss: 6.6630\n",
      "Epoch 853/1000\n",
      "9081/9081 [==============================] - 1s 158us/step - loss: 5.7205 - val_loss: 18.7338\n",
      "Epoch 854/1000\n",
      "9081/9081 [==============================] - 1s 160us/step - loss: 5.1216 - val_loss: 8.3446\n",
      "Epoch 855/1000\n",
      "9081/9081 [==============================] - 1s 145us/step - loss: 5.8396 - val_loss: 8.1050\n",
      "Epoch 856/1000\n",
      "9081/9081 [==============================] - 1s 143us/step - loss: 6.8777 - val_loss: 6.3329\n",
      "Epoch 857/1000\n",
      "9081/9081 [==============================] - 1s 164us/step - loss: 5.1512 - val_loss: 6.9773\n",
      "Epoch 858/1000\n",
      "9081/9081 [==============================] - 1s 164us/step - loss: 5.5771 - val_loss: 5.4978\n",
      "Epoch 859/1000\n",
      "9081/9081 [==============================] - 1s 165us/step - loss: 6.8518 - val_loss: 4.3172\n",
      "Epoch 860/1000\n",
      "9081/9081 [==============================] - 1s 162us/step - loss: 5.6357 - val_loss: 9.7168\n",
      "Epoch 861/1000\n",
      "9081/9081 [==============================] - 2s 165us/step - loss: 5.0049 - val_loss: 7.2255\n",
      "Epoch 862/1000\n",
      "9081/9081 [==============================] - 1s 140us/step - loss: 5.9710 - val_loss: 7.2449\n",
      "Epoch 863/1000\n",
      "9081/9081 [==============================] - 1s 147us/step - loss: 6.1846 - val_loss: 6.2678\n",
      "Epoch 864/1000\n",
      "9081/9081 [==============================] - 1s 154us/step - loss: 5.3334 - val_loss: 9.2671\n",
      "Epoch 865/1000\n",
      "9081/9081 [==============================] - 1s 136us/step - loss: 6.0471 - val_loss: 10.5896\n",
      "Epoch 866/1000\n",
      "9081/9081 [==============================] - 1s 161us/step - loss: 5.8634 - val_loss: 3.0475\n",
      "Epoch 867/1000\n",
      "9081/9081 [==============================] - 1s 160us/step - loss: 5.0834 - val_loss: 6.7784\n",
      "Epoch 868/1000\n",
      "9081/9081 [==============================] - 859s 95ms/step - loss: 6.1540 - val_loss: 12.2189\n",
      "Epoch 869/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 6.2848 - val_loss: 8.7320\n",
      "Epoch 870/1000\n",
      "9081/9081 [==============================] - 1s 57us/step - loss: 6.9299 - val_loss: 4.9943\n",
      "Epoch 871/1000\n",
      "9081/9081 [==============================] - 1s 58us/step - loss: 6.0097 - val_loss: 3.7546\n",
      "Epoch 872/1000\n",
      "9081/9081 [==============================] - 1s 80us/step - loss: 5.0056 - val_loss: 8.3719\n",
      "Epoch 873/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 4.7359 - val_loss: 9.7782\n",
      "Epoch 874/1000\n",
      "9081/9081 [==============================] - 1s 56us/step - loss: 4.9487 - val_loss: 4.6863\n",
      "Epoch 875/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.1511 - val_loss: 6.4724\n",
      "Epoch 876/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 4.9144 - val_loss: 3.0305\n",
      "Epoch 877/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 6.5517 - val_loss: 6.8780\n",
      "Epoch 878/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.7386 - val_loss: 8.7202\n",
      "Epoch 879/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 6.2459 - val_loss: 5.9215\n",
      "Epoch 880/1000\n",
      "9081/9081 [==============================] - 1s 87us/step - loss: 5.8417 - val_loss: 11.1083\n",
      "Epoch 881/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.1995 - val_loss: 7.6265\n",
      "Epoch 882/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 5.0307 - val_loss: 9.8601\n",
      "Epoch 883/1000\n",
      "9081/9081 [==============================] - 1s 83us/step - loss: 5.1062 - val_loss: 10.3008\n",
      "Epoch 884/1000\n",
      "9081/9081 [==============================] - 1s 94us/step - loss: 6.3891 - val_loss: 3.1305\n",
      "Epoch 885/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 4.0879 - val_loss: 16.3076\n",
      "Epoch 886/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 5.3537 - val_loss: 12.2051\n",
      "Epoch 887/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.2182 - val_loss: 9.7026\n",
      "Epoch 888/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.3609 - val_loss: 4.9163\n",
      "Epoch 889/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.1605 - val_loss: 4.9216\n",
      "Epoch 890/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 5.1410 - val_loss: 5.1119\n",
      "Epoch 891/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 5.5670 - val_loss: 3.3957\n",
      "Epoch 892/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 4.7627 - val_loss: 6.1875\n",
      "Epoch 893/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.0950 - val_loss: 8.9366\n",
      "Epoch 894/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.0742 - val_loss: 6.4155\n",
      "Epoch 895/1000\n",
      "9081/9081 [==============================] - 1s 65us/step - loss: 5.5317 - val_loss: 28.9497\n",
      "Epoch 896/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 4.7884 - val_loss: 4.6470\n",
      "Epoch 897/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 6.2106 - val_loss: 9.6262\n",
      "Epoch 898/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 6.2847 - val_loss: 3.7904\n",
      "Epoch 899/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081/9081 [==============================] - 1s 71us/step - loss: 4.8070 - val_loss: 9.0745\n",
      "Epoch 900/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 5.8181 - val_loss: 12.1040\n",
      "Epoch 901/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 5.4992 - val_loss: 22.7855\n",
      "Epoch 902/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.6220 - val_loss: 6.9223\n",
      "Epoch 903/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.4602 - val_loss: 29.9560\n",
      "Epoch 904/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.1467 - val_loss: 2.3858\n",
      "Epoch 905/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.2440 - val_loss: 7.2303\n",
      "Epoch 906/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.2684 - val_loss: 7.4643\n",
      "Epoch 907/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.3767 - val_loss: 2.2844\n",
      "Epoch 908/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.3136 - val_loss: 4.2243\n",
      "Epoch 909/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 4.7694 - val_loss: 6.8914\n",
      "Epoch 910/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 4.9232 - val_loss: 2.8569\n",
      "Epoch 911/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.1020 - val_loss: 10.0788\n",
      "Epoch 912/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.3725 - val_loss: 5.2936\n",
      "Epoch 913/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 4.4624 - val_loss: 7.3412\n",
      "Epoch 914/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.9485 - val_loss: 17.6590\n",
      "Epoch 915/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 4.7814 - val_loss: 10.4024\n",
      "Epoch 916/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.6969 - val_loss: 17.4514\n",
      "Epoch 917/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 6.3065 - val_loss: 5.7028\n",
      "Epoch 918/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 5.4968 - val_loss: 38.8119\n",
      "Epoch 919/1000\n",
      "9081/9081 [==============================] - 1s 67us/step - loss: 5.6629 - val_loss: 7.4186\n",
      "Epoch 920/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 5.4714 - val_loss: 7.7684\n",
      "Epoch 921/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 4.5782 - val_loss: 12.9450\n",
      "Epoch 922/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.5478 - val_loss: 14.3110\n",
      "Epoch 923/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 6.1949 - val_loss: 5.8853\n",
      "Epoch 924/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 4.6709 - val_loss: 12.1116\n",
      "Epoch 925/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 6.0336 - val_loss: 7.2170\n",
      "Epoch 926/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.3956 - val_loss: 7.9479\n",
      "Epoch 927/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 4.9308 - val_loss: 5.8344\n",
      "Epoch 928/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 4.5494 - val_loss: 11.3178\n",
      "Epoch 929/1000\n",
      "9081/9081 [==============================] - 1s 83us/step - loss: 4.8863 - val_loss: 6.4581\n",
      "Epoch 930/1000\n",
      "9081/9081 [==============================] - 1s 86us/step - loss: 5.1572 - val_loss: 3.1337\n",
      "Epoch 931/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 4.9549 - val_loss: 7.8054\n",
      "Epoch 932/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 6.0758 - val_loss: 7.9416\n",
      "Epoch 933/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 5.4447 - val_loss: 13.9310\n",
      "Epoch 934/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 4.8987 - val_loss: 14.1769\n",
      "Epoch 935/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.7332 - val_loss: 3.7459\n",
      "Epoch 936/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.3802 - val_loss: 6.9298\n",
      "Epoch 937/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 4.8618 - val_loss: 8.2241\n",
      "Epoch 938/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 5.1861 - val_loss: 5.9767\n",
      "Epoch 939/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.8955 - val_loss: 7.0839\n",
      "Epoch 940/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 6.1498 - val_loss: 19.9972\n",
      "Epoch 941/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.1009 - val_loss: 3.3480\n",
      "Epoch 942/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.4155 - val_loss: 2.8092\n",
      "Epoch 943/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.2640 - val_loss: 20.2369\n",
      "Epoch 944/1000\n",
      "9081/9081 [==============================] - 1s 83us/step - loss: 4.9574 - val_loss: 5.7071\n",
      "Epoch 945/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 4.8173 - val_loss: 7.0869\n",
      "Epoch 946/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.1519 - val_loss: 21.1774\n",
      "Epoch 947/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.7107 - val_loss: 7.1151\n",
      "Epoch 948/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.4143 - val_loss: 7.3691\n",
      "Epoch 949/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 4.8930 - val_loss: 7.4950\n",
      "Epoch 950/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.3338 - val_loss: 2.6906\n",
      "Epoch 951/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.0615 - val_loss: 8.2603\n",
      "Epoch 952/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.4592 - val_loss: 7.9854\n",
      "Epoch 953/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.3324 - val_loss: 9.9667\n",
      "Epoch 954/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.8994 - val_loss: 3.7009\n",
      "Epoch 955/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 6.0959 - val_loss: 3.2077\n",
      "Epoch 956/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 4.1929 - val_loss: 12.7414\n",
      "Epoch 957/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.2763 - val_loss: 9.6904\n",
      "Epoch 958/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 4.6914 - val_loss: 2.8251\n",
      "Epoch 959/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 4.7013 - val_loss: 9.1029\n",
      "Epoch 960/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.5402 - val_loss: 13.5860\n",
      "Epoch 961/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 4.2357 - val_loss: 3.1741\n",
      "Epoch 962/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 5.5404 - val_loss: 3.3145\n",
      "Epoch 963/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 5.3870 - val_loss: 3.6585\n",
      "Epoch 964/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 5.8660 - val_loss: 10.6786\n",
      "Epoch 965/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 4.4571 - val_loss: 3.5468\n",
      "Epoch 966/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 5.9649 - val_loss: 7.3369\n",
      "Epoch 967/1000\n",
      "9081/9081 [==============================] - 1s 71us/step - loss: 5.8305 - val_loss: 6.2363\n",
      "Epoch 968/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 5.5265 - val_loss: 14.6167\n",
      "Epoch 969/1000\n",
      "9081/9081 [==============================] - 1s 74us/step - loss: 5.0408 - val_loss: 5.3821\n",
      "Epoch 970/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 4.2504 - val_loss: 20.9766\n",
      "Epoch 971/1000\n",
      "9081/9081 [==============================] - 1s 73us/step - loss: 6.2275 - val_loss: 8.8385\n",
      "Epoch 972/1000\n",
      "9081/9081 [==============================] - 1s 72us/step - loss: 4.5946 - val_loss: 5.3972\n",
      "Epoch 973/1000\n",
      "9081/9081 [==============================] - 1s 79us/step - loss: 5.4913 - val_loss: 8.9760\n",
      "Epoch 974/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.3952 - val_loss: 10.0084\n",
      "Epoch 975/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 5.0666 - val_loss: 6.6136\n",
      "Epoch 976/1000\n",
      "9081/9081 [==============================] - 1s 68us/step - loss: 4.9212 - val_loss: 7.4663\n",
      "Epoch 977/1000\n",
      "9081/9081 [==============================] - 1s 70us/step - loss: 4.9090 - val_loss: 9.7924\n",
      "Epoch 978/1000\n",
      "9081/9081 [==============================] - 1s 69us/step - loss: 4.5542 - val_loss: 6.6432\n",
      "Epoch 979/1000\n",
      "9081/9081 [==============================] - 1s 91us/step - loss: 5.2792 - val_loss: 21.2552\n",
      "Epoch 980/1000\n",
      "9081/9081 [==============================] - 1s 89us/step - loss: 5.8490 - val_loss: 6.9880\n",
      "Epoch 981/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 4.8014 - val_loss: 11.2428\n",
      "Epoch 982/1000\n",
      "9081/9081 [==============================] - 1s 90us/step - loss: 5.7280 - val_loss: 5.4523\n",
      "Epoch 983/1000\n",
      "9081/9081 [==============================] - 1s 115us/step - loss: 4.1562 - val_loss: 13.9506\n",
      "Epoch 984/1000\n",
      "9081/9081 [==============================] - 1s 113us/step - loss: 4.8213 - val_loss: 4.1197\n",
      "Epoch 985/1000\n",
      "9081/9081 [==============================] - 1s 110us/step - loss: 5.1728 - val_loss: 7.2612\n",
      "Epoch 986/1000\n",
      "9081/9081 [==============================] - 1s 112us/step - loss: 4.8884 - val_loss: 7.9953\n",
      "Epoch 987/1000\n",
      "9081/9081 [==============================] - 1s 111us/step - loss: 5.1802 - val_loss: 4.2877\n",
      "Epoch 988/1000\n",
      "9081/9081 [==============================] - 1s 114us/step - loss: 5.1122 - val_loss: 7.7899\n",
      "Epoch 989/1000\n",
      "9081/9081 [==============================] - 1s 81us/step - loss: 4.5091 - val_loss: 3.7455\n",
      "Epoch 990/1000\n",
      "9081/9081 [==============================] - 1s 84us/step - loss: 5.0840 - val_loss: 13.8604\n",
      "Epoch 991/1000\n",
      "9081/9081 [==============================] - 1s 84us/step - loss: 4.6591 - val_loss: 7.2505\n",
      "Epoch 992/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 5.4036 - val_loss: 8.3777\n",
      "Epoch 993/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 5.4434 - val_loss: 4.1332\n",
      "Epoch 994/1000\n",
      "9081/9081 [==============================] - 1s 78us/step - loss: 4.7582 - val_loss: 3.5057\n",
      "Epoch 995/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 5.3383 - val_loss: 8.2090\n",
      "Epoch 996/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 5.6326 - val_loss: 6.9085\n",
      "Epoch 997/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 3.9212 - val_loss: 13.6017\n",
      "Epoch 998/1000\n",
      "9081/9081 [==============================] - 1s 77us/step - loss: 4.7516 - val_loss: 4.1974\n",
      "Epoch 999/1000\n",
      "9081/9081 [==============================] - 1s 76us/step - loss: 4.6745 - val_loss: 7.1746\n",
      "Epoch 1000/1000\n",
      "9081/9081 [==============================] - 1s 75us/step - loss: 4.8418 - val_loss: 6.9133\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(BatchNormalization(input_shape=[7]))\n",
    "model.add(Dense(256, input_shape = [X_train.shape[1]], activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(1, activation ='relu'))\n",
    "model.compile(optimizer='RMSProp', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(x=X_train,y=Y_train, epochs=1000, batch_size=128, validation_split=0.2, verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and cross validation mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x257b9e51940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'],loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2838/2838 [==============================] - 0s 71us/step\n",
      "1.964294451319726\n"
     ]
    }
   ],
   "source": [
    "loss=model.evaluate(x=X_test,y=Y_test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x257bb252780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Y_test,predicted,'o')\n",
    "plt.plot(Y_test,f(Y_test),'r-')\n",
    "plt.title('Real market price VS. Model predicted price')\n",
    "plt.xlabel('Real market price')\n",
    "plt.ylabel('Model predicted price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 matches\n",
      "\n",
      "Real Market Price is: [20.64999962]\n",
      "Model Predicted Price is: [20.649878]\n",
      "\n",
      "Real Market Price is: [2.84000015]\n",
      "Model Predicted Price is: [2.8402543]\n",
      "\n",
      "Real Market Price is: [33.57500076]\n",
      "Model Predicted Price is: [33.575542]\n",
      "\n",
      "Real Market Price is: [43.79999924]\n",
      "Model Predicted Price is: [43.79839]\n",
      "\n",
      "Real Market Price is: [0.21000001]\n",
      "Model Predicted Price is: [0.20825386]\n",
      "\n",
      "Real Market Price is: [69.09999847]\n",
      "Model Predicted Price is: [69.09789]\n",
      "\n",
      "Real Market Price is: [18.25]\n",
      "Model Predicted Price is: [18.247805]\n",
      "\n",
      "Real Market Price is: [33.125]\n",
      "Model Predicted Price is: [33.127388]\n",
      "\n",
      "Real Market Price is: [1.04999995]\n",
      "Model Predicted Price is: [1.0524255]\n",
      "\n",
      "Real Market Price is: [32.55000305]\n",
      "Model Predicted Price is: [32.54751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff = {}\n",
    "for i,pred in enumerate(predicted):\n",
    "    diff[i]=abs(pred-Y_test[i])\n",
    "diff = zip(diff.values(),diff.keys())\n",
    "top10 = sorted(diff)[:10]\n",
    "print(\"Top 10 matches\\n\")\n",
    "for item in top10:\n",
    "    ind = item[1]\n",
    "    print(\"Real Market Price is: {}\".format(Y_test[ind]))\n",
    "    print(\"Model Predicted Price is: {}\\n\".format(predicted[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst 10 matches\n",
      "\n",
      "Real Market Price is: [100.375]\n",
      "Model Predicted Price is: [69.83986]\n",
      "\n",
      "Real Market Price is: [31.70000076]\n",
      "Model Predicted Price is: [16.190588]\n",
      "\n",
      "Real Market Price is: [26.70000076]\n",
      "Model Predicted Price is: [11.285929]\n",
      "\n",
      "Real Market Price is: [21.70000076]\n",
      "Model Predicted Price is: [7.213727]\n",
      "\n",
      "Real Market Price is: [41.95000076]\n",
      "Model Predicted Price is: [31.978886]\n",
      "\n",
      "Real Market Price is: [46.57499695]\n",
      "Model Predicted Price is: [37.17508]\n",
      "\n",
      "Real Market Price is: [15.65000057]\n",
      "Model Predicted Price is: [24.60133]\n",
      "\n",
      "Real Market Price is: [13.97500038]\n",
      "Model Predicted Price is: [22.848507]\n",
      "\n",
      "Real Market Price is: [36.45000076]\n",
      "Model Predicted Price is: [27.598911]\n",
      "\n",
      "Real Market Price is: [9.52499962]\n",
      "Model Predicted Price is: [0.67685443]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff = {}\n",
    "for i,pred in enumerate(predicted):\n",
    "    diff[i]=abs(pred-Y_test[i])\n",
    "diff = zip(diff.values(),diff.keys())\n",
    "worst10 = sorted(diff,reverse=True)[:10]\n",
    "print(\"Worst 10 matches\\n\")\n",
    "for item in worst10:\n",
    "    ind = item[1]\n",
    "    print(\"Real Market Price is: {}\".format(Y_test[ind]))\n",
    "    print(\"Model Predicted Price is: {}\\n\".format(predicted[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
